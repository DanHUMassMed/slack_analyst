{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create EndNote Import of References\n",
    "\n",
    "This code will processes a directory of pdf files and creates a bibliography reference file for import into EndNote 21\n",
    "\n",
    "* __EndNote Import Instructions__\n",
    "    * 1. Given the Output file of this run \"<some_name>.nbib\"\n",
    "    * 2. Open EndNote21 \n",
    "            * File->Importâ€¦ (Select File)\n",
    "    * 3. If prompted, \n",
    "            * _Choose a Filter_\n",
    "            * Select filter: __BibTex__\n",
    "    * 4. The References should now be in your EndNote Library\n",
    "\n",
    "\n",
    "* Before running this notebook On __Watson__\n",
    "    * Start grobid server to get references\n",
    "        * `cd Code/Java/grobid`\n",
    "        * `./gradlew run`\n",
    "        * Note: When starting the service will never reach 100% but it will still be running!\n",
    "    * Start NGINX danhiggins.org to lookup pdfs\n",
    "        * `cd /var/www/danhiggins.org/data`\n",
    "        * copy any need pdf files into this directory\n",
    "        * NGINX is not used to create the reference but if running the pdfs will be available for lookup from endnote notes in the reference\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import re\n",
    "import hashlib\n",
    "from urllib.parse import quote\n",
    "\n",
    "def generate_unique_hash(file_path):\n",
    "    hash_object = hashlib.sha256(file_path.encode())    \n",
    "    return hash_object.hexdigest()\n",
    "\n",
    "def process_header(file_path):\n",
    "    \"\"\"\n",
    "    Using the pdf content use AI to find the bibliography reference data\n",
    "    See: https://grobid.readthedocs.io/en/latest/\n",
    "    \"\"\"\n",
    "    url = 'http://192.168.1.101:8070/api/processHeaderDocument'\n",
    "    headers = {\n",
    "        'Accept': 'application/x-bibtex'\n",
    "    }\n",
    "    files = {\n",
    "        'input': open(file_path, 'rb')\n",
    "    }\n",
    "\n",
    "    response = requests.post(url, headers=headers, files=files)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        return response.text\n",
    "    else:\n",
    "        print(\"Failed to upload file\")\n",
    "        print(\"Status code:\", response.status_code)\n",
    "        return \"\"\n",
    "        \n",
    "\n",
    "\n",
    "def bibtex_to_json(bibtex_str, paper_url):\n",
    "    \"\"\"Convert the bibliography text from grobid into a dictionary\n",
    "\n",
    "    Args:\n",
    "        bibtex_str (str): bibliography text from grobid\n",
    "        paper_url (str): pdf file name\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the Input text can not be parsed\n",
    "\n",
    "    Returns:\n",
    "        dict: The bibliography text as a dictionary\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Remove the @misc{ and } at the start and end\n",
    "        if not bibtex_str.startswith('@misc{'):\n",
    "            raise ValueError(\"Input does not start with '@misc{'\")\n",
    "        bibtex_str = bibtex_str.strip()[6:].strip()\n",
    "        if not bibtex_str.endswith('}'):\n",
    "            raise ValueError(\"Input does not end with '}'\")\n",
    "        bibtex_str = bibtex_str[:-1].strip()\n",
    "\n",
    "        # Split the key and the fields\n",
    "        try:\n",
    "            key, fields_str = bibtex_str.split(',', 1)\n",
    "        except ValueError:\n",
    "            raise ValueError(\"Input does not contain a valid key and fields\")\n",
    "\n",
    "        # Extract fields\n",
    "        fields = re.findall(r'\\s*(\\w+)\\s*=\\s*{(.*?)}\\s*(?:,|$)', fields_str, re.DOTALL)\n",
    "\n",
    "        if not fields:\n",
    "            raise ValueError(\"No valid fields found in input\")\n",
    "\n",
    "        # Convert to dictionary\n",
    "        encoded_paper_url = quote(paper_url)\n",
    "        bibtex_dict = {\"type\"   : \"misc\", \n",
    "                       \"key\"    : key.strip(), \n",
    "                       \"pdf_url\": f\"https://danhiggins.org/data/{encoded_paper_url}\",\n",
    "                       \"uuid\"   : generate_unique_hash(paper_url)}\n",
    "        for field in fields:\n",
    "            field_name = field[0].strip()\n",
    "            field_value = field[1].strip().replace('\\n', ' ')\n",
    "            bibtex_dict[field_name] = field_value\n",
    "\n",
    "        return bibtex_dict\n",
    "    except Exception as e:\n",
    "        return {\"error\": str(e)}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"title\": \"Lipid and Carbohydrate Metabolism in Caenorhabditis elegans\",\n",
      "    \"authors\": \"\",\n",
      "    \"journal\": \"Genetics\",\n",
      "    \"year\": 2023,\n",
      "    \"volume\": \"N/A\",\n",
      "    \"issue\": \"N/A\",\n",
      "    \"page\": \"N/A\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from habanero import Crossref\n",
    "\n",
    "def cross_ref(doi):\n",
    "    \"\"\"Look up the bibliography given that you have a doi reference\n",
    "    Note: Crossref provides additional information not available from grobid\n",
    "    but not all bibliographies have a doi reference\n",
    "\n",
    "    Args:\n",
    "        doi (URL): doi URL to a document\n",
    "\n",
    "    Returns:\n",
    "        dict: The bibliography text as a dictionary\n",
    "    \"\"\"\n",
    "    citation = {}\n",
    "    chars_to_remove = \"(){}[]\"\n",
    "    # Create a translation table that maps each chars_to_remove to None\n",
    "    translation_table = str.maketrans('', '', chars_to_remove)\n",
    "\n",
    "    # Remove the special characters from the input\n",
    "    doi = doi.translate(translation_table)\n",
    "\n",
    "    cr = Crossref()\n",
    "    try:\n",
    "        metadata = cr.works(ids=doi)\n",
    "        # Extract citation information\n",
    "        citation['title'] = metadata['message']['title'][0]\n",
    "        authors_lst = [author.get('given','') + \" \" + author.get('family','') for author in metadata['message'].get('author','')]\n",
    "        citation['authors'] = ' and '.join(authors_lst)\n",
    "        container_title = metadata['message'].get('container-title',[])\n",
    "        if container_title:\n",
    "            citation['journal'] = container_title[0]\n",
    "        citation['year'] = metadata['message']['created']['date-parts'][0][0]\n",
    "        citation['volume'] = metadata['message'].get('volume', 'N/A')\n",
    "        citation['issue'] = metadata['message'].get('issue', 'N/A')\n",
    "        citation['page'] = metadata['message'].get('page', 'N/A')\n",
    "    except Exception as e:\n",
    "        print(\"Failed to process:\", e)\n",
    "    \n",
    "    return citation\n",
    "    # Format the citation\n",
    "\n",
    "# Define the DOI\n",
    "doi = \"10.1534/genetics.117.300106\" # Right\n",
    "print(json.dumps(cross_ref(doi),indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity: 0.22028815056182968\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def similarity_score(str1, str2):\n",
    "    \"\"\"Given two string return a score between 0 and 1 the represents how similar the strings are to eachother \n",
    "    NOTE: In this code similarity_score is used to compare the likeness of two titles  \n",
    "    \"\"\"\n",
    "    documents = [str1, str2]\n",
    "\n",
    "    # Convert the strings to vectors\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(documents)\n",
    "\n",
    "    # Compute cosine similarity\n",
    "    cosine_sim = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])\n",
    "    return cosine_sim[0][0]\n",
    "\n",
    "\n",
    "str1 = \"I love programming\"\n",
    "str2 = \"Programming is my passion\"\n",
    "score = similarity_score(str1,str2)\n",
    "print(f\"Cosine similarity: {score}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_nbib(papers, out_file):\n",
    "    \"\"\"Give a List of papers convert to a format that can be imported into endnote\n",
    "\n",
    "    Args:\n",
    "        papers (list): A list of dictionaries representing articles\n",
    "        out_file (str): the endnote import file name\n",
    "    \"\"\"\n",
    "    bib_lex_lib=\"\"\n",
    "    for article in papers:\n",
    "        bib_lex = (\n",
    "            f\"@Article{{\\n\"\n",
    "            f\"author ={{{article.get('author', '')}}},\\n\"\n",
    "            f\"title ={{{article.get('title', '')}}},\\n\"\n",
    "            f\"year ={{{article.get('year', '')}}},\\n\"\n",
    "            f\"doi ={{{article.get('doi', '')}}},\\n\"\n",
    "            f\"abstract ={{{article.get('abstract', '')}}},\\n\"\n",
    "            f\"keywords ={{{article.get('keywords', '')}}},\\n\"\n",
    "            f\"url ={{{article.get('pdf_url', '')}}},\\n\"\n",
    "            f\"journal ={{{article.get('journal', '')}}},\\n\"\n",
    "            f\"volume ={{{article.get('volume', '')}}},\\n\"\n",
    "            f\"pages ={{{article.get('pages', '')}}},\\n\"\n",
    "            f\"source={{{article.get('uuid', '')}}}\\n\"            \n",
    "            f\"}}\\n\\n\"\n",
    "        )\n",
    "        bib_lex_lib += bib_lex\n",
    "    \n",
    "    with open(out_file, 'w') as file:\n",
    "        file.write(bib_lex_lib)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "\n",
    "def get_last_directory_and_file(file_path):\n",
    "    # Get the directory name from the file path\n",
    "    directory = os.path.dirname(file_path)\n",
    "    \n",
    "    # Get the file name from the file path\n",
    "    file_name = os.path.basename(file_path)\n",
    "    \n",
    "    # Get the last directory name\n",
    "    last_directory = os.path.basename(directory)\n",
    "    \n",
    "    # Combine the last directory and the file name\n",
    "    result = os.path.join(last_directory, file_name)\n",
    "    \n",
    "    return result\n",
    "\n",
    "def process_pdfs_in_directory(directory_path, endnote_dict):\n",
    "    unprocessed_files = []\n",
    "    # Check if the directory exists\n",
    "    if not os.path.isdir(directory_path):\n",
    "        print(f\"The directory {directory_path} does not exist.\")\n",
    "        return\n",
    "\n",
    "    # List all files in the directory\n",
    "    processed = 0\n",
    "    file_list = os.listdir(directory_path)\n",
    "    num_files = len(file_list)\n",
    "    for filename in file_list:\n",
    "        # Construct the full file path\n",
    "        file_path = os.path.join(directory_path, filename)\n",
    "        \n",
    "        # Check if the file is a PDF\n",
    "        if os.path.isfile(file_path) and filename.lower().endswith('.pdf'):\n",
    "            # Process the PDF file\n",
    "            last_directory_and_file = get_last_directory_and_file(file_path)\n",
    "            uuid = generate_unique_hash(last_directory_and_file)\n",
    "            if not uuid in endnote_dict:\n",
    "                unprocessed_files.append(file_path)\n",
    "    return unprocessed_files\n",
    "\n",
    "def process_pdfs_in_directory1(directory_path):\n",
    "    papers = []\n",
    "    # Check if the directory exists\n",
    "    if not os.path.isdir(directory_path):\n",
    "        print(f\"The directory {directory_path} does not exist.\")\n",
    "        return\n",
    "\n",
    "    # List all files in the directory\n",
    "    processed = 0\n",
    "    file_list = os.listdir(directory_path)\n",
    "    num_files = len(file_list)\n",
    "    for filename in file_list:\n",
    "        # Construct the full file path\n",
    "        file_path = os.path.join(directory_path, filename)\n",
    "        \n",
    "        # Check if the file is a PDF\n",
    "        if os.path.isfile(file_path) and filename.lower().endswith('.pdf'):\n",
    "            # Process the PDF file\n",
    "            print(file_path)\n",
    "            bibtex_str = process_header(file_path)\n",
    "            last_directory_and_file = get_last_directory_and_file(file_path)\n",
    "            \n",
    "            bibtex_json = bibtex_to_json(bibtex_str, last_directory_and_file)\n",
    "            if 'doi' in bibtex_json:\n",
    "                citation = cross_ref(bibtex_json['doi'])\n",
    "                #print(f\"{citation=}\")\n",
    "                if 'title' in bibtex_json and 'title' in citation:\n",
    "                    score = similarity_score(citation['title'], bibtex_json['title'])\n",
    "                    bibtex_json['score'] = score\n",
    "                    if score > 0.9:\n",
    "                        bibtex_json['title'] = citation.get('title','')\n",
    "                        bibtex_json['author'] = citation.get('authors','')\n",
    "                        bibtex_json['journal'] = citation.get('journal','')\n",
    "                        bibtex_json['volume'] = citation.get('volume','')\n",
    "                        bibtex_json['pages'] = citation.get('page','')\n",
    "                        bibtex_json['year'] = citation.get('year','')\n",
    "                elif not 'title' in bibtex_json and 'title' in citation:\n",
    "                    # If we dont have a title but we do have a doi\n",
    "                    # Check if the url and the doi are similar\n",
    "                    # If they are then assume we have a match\n",
    "                    print(\"No title but we do have a doi\")\n",
    "                    bibtex_doi = bibtex_json['doi']\n",
    "                    bibtex_url = bibtex_json['pdf_url']\n",
    "                    doi_six = bibtex_doi[-6:]\n",
    "                    if len(bibtex_url) > 4:\n",
    "                        url_six = bibtex_url[:-4]\n",
    "                    else:\n",
    "                        url_six = ''\n",
    "                    url_six = url_six[-6:]\n",
    "                    score = similarity_score(doi_six, url_six)\n",
    "                    if score > 0.9:\n",
    "                        print(\"Found a match for no title!!\")\n",
    "                        bibtex_json['title'] = citation.get('title','')\n",
    "                        bibtex_json['author'] = citation.get('authors','')\n",
    "                        bibtex_json['journal'] = citation.get('journal','')\n",
    "                        bibtex_json['volume'] = citation.get('volume','')\n",
    "                        bibtex_json['pages'] = citation.get('page','')\n",
    "                        bibtex_json['year'] = citation.get('year','')\n",
    "                    \n",
    "            processed +=1\n",
    "            print(f\"{filename} {processed:03d} of {num_files:03d}\")\n",
    "            papers.append(bibtex_json)\n",
    "            #if processed > 10:\n",
    "            #    break\n",
    "            \n",
    "    return papers\n",
    "\n",
    "def write_json_to_file(json_data, file_path):\n",
    "    with open(file_path, 'w') as json_file:\n",
    "        json.dump(json_data, json_file, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import uuid\n",
    "\n",
    "def parse_reference_str(references_str):\n",
    "    \"\"\"\n",
    "    Parse a single reference entry into a dictionary.\n",
    "    \"\"\"\n",
    "    lines = references_str.strip().split('\\n')\n",
    "    reference_dict = {}\n",
    "    \n",
    "    for line in lines:\n",
    "        if \": \" in line:\n",
    "            key, value = line.split(\": \", 1)\n",
    "            reference_dict[key.strip()] = value.strip()\n",
    "    \n",
    "    return reference_dict\n",
    "\n",
    "def read_endnote_lib_file(file_path):\n",
    "    \"\"\"\n",
    "    Note: EndNote Export using Output Style \"Show All Fields\"\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8-sig') as file:\n",
    "        content = file.read()\n",
    "    \n",
    "    # Split content into individual references\n",
    "    references_str = content.strip().split('\\n\\n')\n",
    "    references_dict = {}\n",
    "    for ref_str in references_str:\n",
    "        ref_dict = parse_reference_str(ref_str)\n",
    "        id = ref_dict.get('Name of Database',str(uuid.uuid4()))\n",
    "        references_dict[id]=ref_dict\n",
    "    \n",
    "    return references_dict\n",
    "\n",
    "# # Specify the file path\n",
    "# file_path = 'My_EndNote_Library.txt'\n",
    "\n",
    "# # Parse the file and get the JSON output\n",
    "# references_dict = {}\n",
    "# references_dict = read_endnote_lib_file(file_path)\n",
    "\n",
    "# # Print the JSON output\n",
    "# json_output = json.dumps(references_dict, indent=4)\n",
    "# print(json_output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "Time: 0 minutes and 0 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "directory_path = '/var/www/danhiggins.org/data/papers_arf_lipid_golgi/'\n",
    "endnote_lib_file = 'My_EndNote_Library.txt'\n",
    "\n",
    "endnote_dict = read_endnote_lib_file(endnote_lib_file)\n",
    "\n",
    "# Call the function to process PDFs in the directory\n",
    "unprocessed_files = process_pdfs_in_directory(directory_path, endnote_dict)\n",
    "\n",
    "print(unprocessed_files)\n",
    "#write_json_to_file(papers, \"./papers.json\")\n",
    "#create_nbib(papers, \"arf_lipid_golgi.nbib\")\n",
    "\n",
    "end_time = time.time()\n",
    "time_difference = end_time - start_time\n",
    "minutes = int(time_difference // 60)\n",
    "seconds = int(time_difference % 60)\n",
    "\n",
    "print(f\"Time: {minutes} minutes and {seconds} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "temp-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
